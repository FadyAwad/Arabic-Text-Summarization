{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":4689800,"sourceType":"datasetVersion","datasetId":2694998}],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### ***Import Libraries***","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nfrom transformers import AdamW, get_scheduler\nfrom datasets import load_metric\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\n\nfrom tqdm.auto import tqdm\n\nimport seaborn as sns\nfrom pylab import rcParams\nimport matplotlib.pyplot as plt\nfrom matplotlib import rc\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n# %matplotlib inline\n# %config InlineBackend.figure_format='retina'\n# sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n# rcParams['figure.figsize'] = 16, 10","metadata":{"execution":{"iopub.status.busy":"2024-03-01T16:42:43.682235Z","iopub.execute_input":"2024-03-01T16:42:43.683104Z","iopub.status.idle":"2024-03-01T16:42:43.689774Z","shell.execute_reply.started":"2024-03-01T16:42:43.683071Z","shell.execute_reply":"2024-03-01T16:42:43.688938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### ***Load Model***","metadata":{}},{"cell_type":"code","source":"from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n\ntokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50\", src_lang=\"ar_AR\", tgt_lang=\"ar_AR\")\nmodel = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50\")","metadata":{"execution":{"iopub.status.busy":"2024-03-01T16:42:48.224698Z","iopub.execute_input":"2024-03-01T16:42:48.225083Z","iopub.status.idle":"2024-03-01T16:43:10.770677Z","shell.execute_reply.started":"2024-03-01T16:42:48.225055Z","shell.execute_reply":"2024-03-01T16:43:10.769767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### ***Load Dataset***","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/arabicsummarization/summarizdataset.csv\")\n\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2024-03-01T16:43:17.984916Z","iopub.execute_input":"2024-03-01T16:43:17.986135Z","iopub.status.idle":"2024-03-01T16:43:18.635939Z","shell.execute_reply.started":"2024-03-01T16:43:17.986089Z","shell.execute_reply":"2024-03-01T16:43:18.634979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.drop('type',inplace=True,axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-03-01T16:44:08.368095Z","iopub.execute_input":"2024-03-01T16:44:08.368478Z","iopub.status.idle":"2024-03-01T16:44:08.380536Z","shell.execute_reply.started":"2024-03-01T16:44:08.368446Z","shell.execute_reply":"2024-03-01T16:44:08.379704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.drop('text',inplace=True,axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-03-01T16:44:10.244563Z","iopub.execute_input":"2024-03-01T16:44:10.245271Z","iopub.status.idle":"2024-03-01T16:44:10.251039Z","shell.execute_reply.started":"2024-03-01T16:44:10.245237Z","shell.execute_reply":"2024-03-01T16:44:10.250059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data","metadata":{"execution":{"iopub.status.busy":"2024-03-01T16:44:14.784721Z","iopub.execute_input":"2024-03-01T16:44:14.785364Z","iopub.status.idle":"2024-03-01T16:44:14.798713Z","shell.execute_reply.started":"2024-03-01T16:44:14.785321Z","shell.execute_reply":"2024-03-01T16:44:14.797740Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### ***Data Preprocessing***","metadata":{}},{"cell_type":"code","source":"# Data Preprocessing\n# First Checking 'duplicated rows' and removing them if they are exist\ndupl = data[data.duplicated()]\nif len(dupl)>0:\n    data=data.drop_duplicates()\n    print(len(dupl))\n    \n# !pip install nltk\nimport re\nimport nltk\n### from nltk.stem.isri import ISRIStemmer\n### st = ISRIStemmer()\n\n# Data cleaning\ndef clean_text(text):\n    # Remove special characters and punctuation and \"Arabic digits\"\n    text = re.sub(r'[^\\u0621-\\u064A \\u0660-\\u0669 0-9 \\s ( ) : . ، ؛ ]+','',text)\n    \n    # Remove extra whitespace\n    text = re.sub('\\s+', ' ', text)\n    text = re.sub(r'\\.{2,}', '', text)\n\n    return text\n\n# Data preprocessing\ndef pre_process(text):\n    tokens = nltk.word_tokenize(text)\n    stop_words = [\"و\", \"في\", \"من\", \"على\", \"إلى\", \"عن\", \"فيه\", \"عليه\", \"هو\", \"هي\"]\n    text = \" \".join([word for word in tokens if word not in stop_words])\n\n    return text\n\ndata['Processed Text'] = data['Processed Text'].apply(clean_text)\n\nprint(data['Processed Text'])\n\n# Apply the clean_text function to the content column\ndata['Processed Text'] = data['Processed Text'].apply(clean_text)\nprint(data['Processed Text'])\n","metadata":{"execution":{"iopub.status.busy":"2024-03-01T16:44:18.439759Z","iopub.execute_input":"2024-03-01T16:44:18.440112Z","iopub.status.idle":"2024-03-01T16:44:20.859928Z","shell.execute_reply.started":"2024-03-01T16:44:18.440083Z","shell.execute_reply":"2024-03-01T16:44:20.858944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nstop_words = set(stopwords.words('arabic'))\npreprocessed_text=[]\ndef preprocess_arabic_Remove_stop(text):\n    \n    # Tokenize text into words\n    words = nltk.word_tokenize(text)\n    \n    # Remove stop words\n    words = [word for word in words if word not in stop_words]\n    \n    # Stem the words\n    #words = [stemmer.stem(word) for word in words]\n    \n    # Join the words back into a single string\n    preprocessed_text = ' '.join(words)\n    \n    return preprocessed_text\n\ndata['Processed Text'] = data['Processed Text'].apply(preprocess_arabic_Remove_stop)\ndata['Processed Text'] = data['Processed Text'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2024-03-01T16:44:26.759039Z","iopub.execute_input":"2024-03-01T16:44:26.759529Z","iopub.status.idle":"2024-03-01T16:44:33.535002Z","shell.execute_reply.started":"2024-03-01T16:44:26.759493Z","shell.execute_reply":"2024-03-01T16:44:33.534045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install rouge_score","metadata":{"execution":{"iopub.status.busy":"2024-03-01T16:44:38.774202Z","iopub.execute_input":"2024-03-01T16:44:38.774839Z","iopub.status.idle":"2024-03-01T16:44:56.095124Z","shell.execute_reply.started":"2024-03-01T16:44:38.774806Z","shell.execute_reply":"2024-03-01T16:44:56.093922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SummaryDataset(Dataset):\n    def __init__(\n        self,\n        data=data,\n        tokenizer=tokenizer,\n        text_max_token_len = 800,\n        summary_max_token_len = 150\n    ):\n        self.tokenizer = tokenizer\n        self.data = data\n        self.text_max_token_len = text_max_token_len\n        self.summary_max_token_len = summary_max_token_len\n    \n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, index: int):\n        data_row = self.data.iloc[index]\n\n        text = data_row['Processed Text']\n\n        text_encoding = tokenizer(\n            text,\n            max_length=self.text_max_token_len,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            add_special_tokens=True,\n            return_tensors='pt'\n        )\n\n        summary_encoding = tokenizer(\n            data_row['summarizer'],\n            max_length=self.summary_max_token_len,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            add_special_tokens=True,\n            return_tensors='pt'\n        )\n        \n        labels = summary_encoding['input_ids']\n        labels[labels == tokenizer.pad_token_id] = -100\n        \n        return dict(\n            input_ids=text_encoding['input_ids'].flatten(),\n            attention_mask=text_encoding['attention_mask'].flatten(),\n            labels=labels.flatten(),\n            decoder_attention_mask=summary_encoding['attention_mask'].flatten()\n        )","metadata":{"execution":{"iopub.status.busy":"2024-03-01T16:44:58.245541Z","iopub.execute_input":"2024-03-01T16:44:58.246356Z","iopub.status.idle":"2024-03-01T16:44:58.256470Z","shell.execute_reply.started":"2024-03-01T16:44:58.246314Z","shell.execute_reply":"2024-03-01T16:44:58.255514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Data Spliting into train and val  \ndf_train, df_test = train_test_split(data, test_size=0.2, random_state=42)\n\ntrain_dataset = SummaryDataset(data=df_train)\ntest_dataset = SummaryDataset(data=df_test)\n\ntrain_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=2)\neval_dataloader = DataLoader(test_dataset, batch_size=2)","metadata":{"execution":{"iopub.status.busy":"2024-03-01T16:45:09.075574Z","iopub.execute_input":"2024-03-01T16:45:09.076457Z","iopub.status.idle":"2024-03-01T16:45:09.096810Z","shell.execute_reply.started":"2024-03-01T16:45:09.076417Z","shell.execute_reply":"2024-03-01T16:45:09.095782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.to_csv('train.csv')\ndf_test.to_csv('val.csv')","metadata":{"execution":{"iopub.status.busy":"2024-03-01T16:45:10.907225Z","iopub.execute_input":"2024-03-01T16:45:10.908124Z","iopub.status.idle":"2024-03-01T16:45:11.243294Z","shell.execute_reply.started":"2024-03-01T16:45:10.908091Z","shell.execute_reply":"2024-03-01T16:45:11.242432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train","metadata":{"execution":{"iopub.status.busy":"2024-03-01T16:45:23.754688Z","iopub.execute_input":"2024-03-01T16:45:23.755316Z","iopub.status.idle":"2024-03-01T16:45:23.768010Z","shell.execute_reply.started":"2024-03-01T16:45:23.755271Z","shell.execute_reply":"2024-03-01T16:45:23.767038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test","metadata":{"execution":{"iopub.status.busy":"2024-03-01T16:45:29.144957Z","iopub.execute_input":"2024-03-01T16:45:29.145857Z","iopub.status.idle":"2024-03-01T16:45:29.157103Z","shell.execute_reply.started":"2024-03-01T16:45:29.145804Z","shell.execute_reply":"2024-03-01T16:45:29.156097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### ***Train mbart Model*** ","metadata":{}},{"cell_type":"code","source":"num_epochs = 4\n\nnum_training_steps = num_epochs * len(train_dataloader)\n\noptimizer = AdamW(model.parameters())\nlr_scheduler = get_scheduler(\n    \"linear\",\n    optimizer=optimizer,\n    num_warmup_steps=0,\n    num_training_steps=num_training_steps\n)\n\nprogress_bar = tqdm(range(num_training_steps))\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\nmodel.train()\nfor epoch in range(num_epochs):\n    for batch in train_dataloader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        \n        outputs = model(**batch)\n#         logits = outputs.logits\n#         predictions = torch.argmax(logits, dim=-1)\n#         print(predictions)\n#         print(batch[\"labels\"])\n        \n        loss = outputs.loss\n        loss.backward()\n        \n        optimizer.step()\n        lr_scheduler.step()\n        \n        optimizer.zero_grad()\n        progress_bar.update()\n    \n    torch.save({\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'loss': loss,\n            }, f'./t5-Arabic.pth')\n\n    print(f'epoch: {epoch + 1} -- loss: {loss}')","metadata":{"execution":{"iopub.status.busy":"2024-03-01T16:45:35.065322Z","iopub.execute_input":"2024-03-01T16:45:35.065995Z","iopub.status.idle":"2024-03-01T20:25:45.417656Z","shell.execute_reply.started":"2024-03-01T16:45:35.065963Z","shell.execute_reply":"2024-03-01T20:25:45.416267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metric= load_metric(\"rouge\")\nmodel.eval()\nfor batch in eval_dataloader:\n    batch = {k: v.to(device) for k, v in batch.items()}\n    with torch.no_grad():\n        outputs = model(**batch)\n\n    logits = outputs.logits\n    predictions = torch.argmax(logits, dim=-1)\n    \n    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n\nmetric.compute()","metadata":{"execution":{"iopub.status.busy":"2024-03-01T20:25:52.630134Z","iopub.execute_input":"2024-03-01T20:25:52.631257Z","iopub.status.idle":"2024-03-01T20:31:08.597232Z","shell.execute_reply.started":"2024-03-01T20:25:52.631212Z","shell.execute_reply":"2024-03-01T20:31:08.596097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def summarizeText(text, model=model):\n    text_encoding = tokenizer(\n        text,\n        max_length=1000,\n        padding='max_length',\n        truncation=True,\n        return_attention_mask=True,\n        add_special_tokens=True,\n        return_tensors='pt'\n    )\n    generated_ids = model.generate(\n        input_ids=text_encoding['input_ids'].to(device),\n        attention_mask=text_encoding['attention_mask'].to(device),\n        max_length=150,\n        num_beams=4,\n        repetition_penalty=2.5,\n        length_penalty=1.0,\n        early_stopping=True\n    )    \n\n    preds = [\n            tokenizer.decode(gen_id, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n            for gen_id in generated_ids\n    ]\n    return \"\".join(preds)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-01T20:48:18.884243Z","iopub.execute_input":"2024-03-01T20:48:18.884663Z","iopub.status.idle":"2024-03-01T20:48:18.893597Z","shell.execute_reply.started":"2024-03-01T20:48:18.884632Z","shell.execute_reply":"2024-03-01T20:48:18.892823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path_input = '/kaggle/working/val.csv'\n\n# Read the CSV file\ndataset = pd.read_csv(path_input)\n\ndataset.head()","metadata":{"execution":{"iopub.status.busy":"2024-03-01T20:48:27.188111Z","iopub.execute_input":"2024-03-01T20:48:27.188485Z","iopub.status.idle":"2024-03-01T20:48:27.243510Z","shell.execute_reply.started":"2024-03-01T20:48:27.188454Z","shell.execute_reply":"2024-03-01T20:48:27.242579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.drop('Unnamed: 0',inplace=True,axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-03-01T20:49:24.799085Z","iopub.execute_input":"2024-03-01T20:49:24.799455Z","iopub.status.idle":"2024-03-01T20:49:24.805203Z","shell.execute_reply.started":"2024-03-01T20:49:24.799425Z","shell.execute_reply":"2024-03-01T20:49:24.804135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset","metadata":{"execution":{"iopub.status.busy":"2024-03-01T20:49:31.459051Z","iopub.execute_input":"2024-03-01T20:49:31.459863Z","iopub.status.idle":"2024-03-01T20:49:31.470196Z","shell.execute_reply.started":"2024-03-01T20:49:31.459831Z","shell.execute_reply":"2024-03-01T20:49:31.469352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Data Preprocessing\n# First Checking 'duplicated rows' and removing them if they are exist\ndupl = dataset[dataset.duplicated()]\nif len(dupl)>0:\n    dataset=dataset.drop_duplicates()\n    print(len(dupl))\n    \n    # Second Checking 'empty cells' and removing them if they are exist\n    # Here we deal with empty cells (either removing them Or setting them with default data) but we won't do that because data is cleaned\n\n    # Third Cleaning data from (Arabic and English digits, special characters, and extra spaces)\nimport re\ndef clean_text(text):\n\n        # Remove special characters and punctuation and \"Arabic digits\"\n    text = re.sub(r'[^\\u0621-\\u064A\\u0660 - \\0669\\s]+', '', text)\n\n        # Remove \"English digits\"\n    text = re.sub('\\d+', '', text)\n\n        # Remove extra whitespace\n    text = re.sub('\\s+', ' ', text).strip()\n\n    return text\n\n# Apply the clean_text function to the content column\ndataset['Processed Text'] = dataset['Processed Text'].apply(clean_text)\ndataset['Processed Text']","metadata":{"execution":{"iopub.status.busy":"2024-03-01T20:49:36.617604Z","iopub.execute_input":"2024-03-01T20:49:36.618492Z","iopub.status.idle":"2024-03-01T20:49:36.793973Z","shell.execute_reply.started":"2024-03-01T20:49:36.618458Z","shell.execute_reply":"2024-03-01T20:49:36.793009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\n# Function to apply the model to each instance\ndef apply_model(instance):\n    # Apply your model to the instance and return the output\n    output = summarizeText(instance)\n    return output\n\n# Create an empty list to store the model outputs\nmodel_outputs = []\n\n# Iterate over the dataframe and apply the model to each instance\nfor index, row in tqdm(dataset.iterrows()):\n    instance = row['Processed Text']  # Assuming 'text' is the column containing the input data\n    output = apply_model(instance)\n    model_outputs.append(output)\n\n# Add the model outputs as a new column to the dataframe\ndataset['summarizer'] = model_outputs\n\n# Display the updated dataframe\nprint(dataset['summarizer'])\n","metadata":{"execution":{"iopub.status.busy":"2024-03-01T21:12:21.753140Z","iopub.execute_input":"2024-03-01T21:12:21.753801Z","iopub.status.idle":"2024-03-01T21:41:56.032916Z","shell.execute_reply.started":"2024-03-01T21:12:21.753769Z","shell.execute_reply":"2024-03-01T21:41:56.031912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Data Preprocessing\n    #First Checking 'duplicated rows' and removing them if they are exist\ndupl = data[data.duplicated()]\nif len(dupl)>0:\n    data=data.drop_duplicates()\n    print(len(dupl))\n    \n    #Second Checking 'empty cells' and removing them if they are exist\n    #Here we deal with empty cells (either removing them Or setting them with default data) but we won't do that because data is cleaned\n\n    #Third Cleaning data from (Arabic and English digits, special characters, and extra spaces)\nimport re\ndef clean_text(text):\n\n        # Remove special characters and punctuation and \"Arabic digits\"\n    text = re.sub(r'[^\\u0621-\\u064A\\u0660 - \\0669\\s]+', '', text)\n\n        # Remove \"English digits\"\n    ##text = re.sub('\\d+', '', text)\n\n        # Remove extra whitespace\n    text = re.sub('\\s+', ' ', text).strip()\n\n    return text\n\n# Apply the clean_text function to the content column\ndataset['summarizer'] = dataset['summarizer'].apply(clean_text)\nprint(dataset['summarizer'])","metadata":{"execution":{"iopub.status.busy":"2024-03-01T21:44:47.191175Z","iopub.execute_input":"2024-03-01T21:44:47.191849Z","iopub.status.idle":"2024-03-01T21:44:47.267664Z","shell.execute_reply.started":"2024-03-01T21:44:47.191815Z","shell.execute_reply":"2024-03-01T21:44:47.266681Z"},"trusted":true},"execution_count":null,"outputs":[]}]}